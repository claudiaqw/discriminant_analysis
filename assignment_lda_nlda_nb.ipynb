{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical assignment Linear Discriminant Analysis, Nonlinear Discriminant Analysis, and Na√Øve Bayes with Scikit Learn\n",
    "### Master in Data Science and Big Data\n",
    "\n",
    "#### Instructor: Irene Rodr√≠guez Luj√°n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we'll go deeper on the fundamentals behind Linear Discriminant Analysis, Nonlinear Discriminant Analysis, and Na√Øve Bayes. Additionally, we will use the Scikit-learn implementation of these methods in a real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideliness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this notebook you will find empty cells that you will need to fill with your own code. Follow the instructions in the notebook and pay special attention to the following symbols.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>You will need to solve a question by writing your own code or answer in the cell immediately below or in a different file, as instructed.</td></tr>\n",
    " <tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>This is a hint or useful observation that can help you solve this assignment. You should pay attention to these hints to better understand the assignment.</td></tr>\n",
    "\n",
    "During the assignment you will make use of several Python packages that might not be installed in your machine. If that is the case, you can install new Python packages with\n",
    "\n",
    "    conda install PACKAGENAME\n",
    "    \n",
    "if you are using Python Anaconda. Else you should use\n",
    "\n",
    "    pip install PACKAGENAME\n",
    "\n",
    "We will use **Python 3.7**. You will need the following packages for this particular assignment. Make sure they are available before proceeding:\n",
    "\n",
    "* **numpy** - 1.19.2\n",
    "* **pandas** - 1.1.3\n",
    "* **matplotlib** - 3.3.4\n",
    "* **scikit-learn** - 0.23.2\n",
    "* **pandas-profiling** - 2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as ma\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes plots appear on the notebook instead of showing them in a new window/tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows packages' versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"numpy: \", np.__version__)\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"matplotlib: \", mpl.__version__)\n",
    "print(\"Scikit-learn: \", skl.__version__)\n",
    "print(\"Pandas profiling: \", pdprof.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if you need any help on the usage of a Python function you can place the writing cursor over its name and press Caps+Shift to produce a pop-out with related documentation. This will only work inside code cells.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One step further on LDA\n",
    "\n",
    "* We will work with the iris dataset. The iris dataset is a classic and very easy multi-class classification dataset; in fact, this is perhaps the best known database to be found in the pattern recognition literature.\n",
    "\n",
    "* The dataset contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n",
    "\n",
    "* Predicted attribute: class of iris plant (iris Setosa, Iris Versicolour, Iris Virginica)\n",
    "\n",
    "* Attributes (predictors): sepal length in cm, sepal width in cm, petal length in cm, petal width in cm\n",
    "\n",
    "The following code uses the `load_iris` function available in scikit learn to load the iris dataset. The `X` variable is a numpy array of dimensions (150,4): 150 samples and 4 attributes. The `y` variable is a numpy vector with 150 entries corresponding to the target for each sample. `y` takes 3 different values corresponding to the three classes of iris plant (setosa, veriscolour, and virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (150, 4)\n",
      "y shape:  (150,)\n",
      "y unique values:  [0 1 2]\n",
      "target names:  ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X=data['data']\n",
    "y=data['target']\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)\n",
    "print(\"y unique values: \", np.unique(y))\n",
    "print(\"target names: \", data['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Train a Linear Discriminant over the iris data using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">Scikit Learn implementation</a> with the default parameters. Print the class priors means, and the covariance matrix computed by the algorithm (to get the covariance matrix you need to set `store_covariance=True`).\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to better understand LDA, we are going to reproduce the computation of class priors and means, and the pooled covariance matrix using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[1 point]</b> Compute the class priors, class means and the pooled covariance matrix $\\Sigma$ using the equations provided in the theoretical part of the course. Print your results, and compare them to the ones provided by de Scikit Learn Function. Some numpy functions that may be useful are <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\">np.sum</a>, <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\">np.mean</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html\">np.cov</a> with parameters rowvar=False,bias=True.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the theoretical part of the course, we learnt that the number of components in LDA is given by min (ùëë, ùê∂‚àí1), where d is the number of dimensions (attributes/predictors) in the dataset, and C is the number of clases. In the iris dataset, we have d=4 and C=3; so, we will expect to get at most 2 dimensions. The default behavior of the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">LinearDiscriminatAnalysis</a> function in Scikit Learn computes all the available components. The following code allows us to determine the number of components as well as the percentage of variance explained by each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Number of components: \", len(clf.explained_variance_ratio_))\n",
    "print(\"*** Percentage explained variance: \",100*clf.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we got 2 components: $2=\\min(d=4,C-1=3-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the course, we learnt that the equation $\\min(d,C-1)$ comes from the rank of $S_{W}^{-1}S_B$, which provides the solution to LDA directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Compute matrix S_B using the equations seen in the theoretical part of the course. Then, compute $S_{W}^{-1}S_B$ (note that you have already calculated $S_W$ in the previous exercise). You can use the numpy function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html\">numpy.matmul</a> for matrix multiplication, and the function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html\"> numpy.linalg.inv</a> for computing the matrix inverse. Finally, compute the rank of $S_{W}^{-1}S_B$ using the function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html\"> numpy.linalg.matrix_rank</a>. Do you get the result you expected?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we're able to reproduce the solution provided by Scikit Learn LDA. The following code shows the normalized eigenvectors (L2 norm = 1) of the solution provided by Scikit Learn's LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "clf = LDA(store_covariance=True, solver='eigen')\n",
    "clf.fit(X, y)\n",
    "norms = norm(clf.scalings_,axis=0)\n",
    "print(\"LDA normalized projections:\")\n",
    "print(clf.scalings_/norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in theory, the projection directions of LDA can be obtained by solving the generalized eigenvalue problem $S_Bw = \\lambda S_W w$, and this is equivalent to find the eigenvalues and eigenvectors of $S_{W}^{-1}S_B$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.25 points]</b> Given the matrix $S_{W}^{-1}S_B$ already computed in the previous exercise, compute its eigenvalues and eigenvectors using the function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html\">numpy.linalg.eig</a>. Read the documentation of this function in order to understand its inputs and outputs. Print the obtained eigenvectors and compare them to the normalized ones obtained from the Scikit Learn's LDA function. Do they match? Are they equivalent as projection directions? Now, normalize the eigenvalues you just have obtained in order to sum up to 1, does the result match with the explained variance provided by the Scikit Learn's LDA function?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to use LDA as a projection direction, and as a predictor. In order to use LDA as a projection direction, we need to use the `transform` function of the Sklearn's LDA object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.25 points]</b> Project the iris data into the LDA subspace using the Scikit Learn's trained classifier. What is the dimension of the resulting matrix? Is it what you expected? Why? Why not?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify the generalization capability of the LDA model, it is desirable to evaluate the predictive capability of the model in a subset of samples not seen during training. In order to split our original iris data into training and test subsets using the Sklearn's function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\"> train_test_split</a>, the following code generates the following objects:\n",
    "* `X_train` and `y_train`: predictors and labels for the training set.\n",
    "* `X_test` and `y_test`: predictors and labels for the test set.\n",
    "\n",
    "80\\% of the whole dataset has been used for training, and the remaining 20\\% has been used for test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.25 points]</b> Train Sklearn's LDA classifier over the training set, and evaluate the classification accuracy over the test set using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict\">LDA's predict function</a>, and the Sklearn function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">accuracy_score</a>. Do you obtain a good result? How does it compare with respect to a random classifier? Try to justify your good/bad results.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One step further on QDA\n",
    "\n",
    "The following code uses the Sklearn's implementation of Quadratic Disciminant Analysis (function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\">QuadraticDiscriminantAnalysis</a>) to train a QDA classifier using the training data for the iris dataset previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "qda.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.75 points]</b> Compute the class priors, class means and the covariance matrices $\\Sigma_i$ using the equations provided in the theoretical part of the course. Print your results, and compare them to the ones provided by de QDA Scikit Learn function. Some numpy functions that may be useful are <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\">np.sum</a>, <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\">np.mean</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html\">np.cov</a> with parameters rowvar=False,bias=False.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Evaluate the classification accuracy over the test set using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict\">QDA's predict function</a>, and the Sklearn function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\">accuracy_score</a>. Do you obtain a good result? How does it compare with respect to a random classifier and LDA? Try to justify your good/bad results.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a couple of non-linear separable synthetic datasets by using the Scikit Learn's functions make_moons and make_circles. These functions generate 2-dimensional binary datasets:\n",
    "\n",
    "* **Moons** (function <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html\">sklearn.datasets.make_moons</a>): Make two interleaving half circles.\n",
    "* **Circles** (funci√≥n <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html\">sklearn.datasets.make_circles</a>): A circle inside another circle.\n",
    "\n",
    "Both datasets are normalized so each varaible has mean 0 and standard deviation 1.\n",
    "\n",
    "Let's take a look at the datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X_moons,y_moons = datasets.make_moons(n_samples=500,noise=0.15,random_state=1)\n",
    "X_circles, y_circles = datasets.make_circles(n_samples=500, noise=0.15,factor=0.5,random_state=1)\n",
    "\n",
    "cm = ma.ListedColormap(['#0000FF', '#FF0000'])    # blue, red\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap=cm)\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap=cm)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a training and test split for each of these datasets using the Scikit's function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\"> train_test_split</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xmoons_train, Xmoons_test, ymoons_train, ymoons_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
    "Xcircles_train, Xcircles_test, ycircles_train, ycircles_test = train_test_split(X_circles, y_circles, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Train LDA and QDA classifiers (Sklearn implementation) over the moons and circles datasets and evaluate classifiers' accuracy over the test sets. Do you obtain a good result? How does it compare with respect to a random classifier and LDA? Try to justify your good/bad results.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `plot_decision_boundary` allows you to visualize the decision boundary of the classifiers. The function has three inputs (in this order):\n",
    "1. Sklearn model\n",
    "2. X\n",
    "3. y\n",
    "\n",
    "The code below shows the decision boundary over the test set of a linearly separable dataset for the LDA and QDA classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_decision_boundary import plot_decision_boundary\n",
    "\n",
    "X_linear, y_linear =  datasets.make_classification(n_samples=500, n_features=2,\n",
    "                                                   n_informative=2, n_redundant=0,\n",
    "                                                   n_clusters_per_class=1, flip_y=-1,\n",
    "                                                  class_sep=2)\n",
    "\n",
    "Xlinear_train, Xlinear_test, ylinear_train, ylinear_test = train_test_split(X_linear, y_linear, test_size=0.2, random_state=42)\n",
    "\n",
    "lda = LDA()\n",
    "lda.fit(Xlinear_train,ylinear_train)\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(Xlinear_train,ylinear_train)\n",
    "\n",
    "plot_decision_boundary(lda,Xlinear_test,ylinear_test)\n",
    "plot_decision_boundary(qda,Xlinear_test,ylinear_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.25 points]</b> Using the function plot_decision_boundary, show the decision boundaries over the test sets of the four classifiers trained in the previous exercise.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One step further on KDA\n",
    "\n",
    "KDA is not implemented in Scikit Learn, so we are going to use an adaptation of an open-source implementation available <a href=\"https://github.com/bhaktipriya/Kernel-PCA-and-LDA/blob/master/klda.py\">here</a>.\n",
    "\n",
    "We are going to consider two kernels: \n",
    "- Linear kernel: no changes are made on the input space.\n",
    "- RBF Kernel: nonlinear transformations on the input space.\n",
    "\n",
    "We'll be working with the circles dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td> Explore the code below and make sure you understand what it is doing.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from klda import klda\n",
    "from sklearn import svm\n",
    "\n",
    "# KDA tranformation to the feature space\n",
    "# classes needs to be {-1,1}\n",
    "print(np.unique(y_circles))\n",
    "y_circles_transf=y_circles\n",
    "y_circles_transf[y_circles==0]=-1\n",
    "X_circles_transf = klda(X_circles, y_circles)\n",
    "\n",
    "\n",
    "# We split the transformed data in training and test\n",
    "Xtransf_train, Xtransf_test, ytransf_train, ytransf_test = train_test_split(X_circles_transf, y_circles_transf, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Train a SVM classifier with RBF kernel (nonlinear classifier)\n",
    "clf = svm.SVC(kernel='rbf', C=1).fit(Xtransf_train, ytransf_train)\n",
    "print(\"% Accuracy rbfsvm\", 100*clf.score(Xtransf_test, ytransf_test))\n",
    "\n",
    "# Train a SVM classifier with a linear kernel (linear classifier)\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(Xtransf_train, ytransf_train)\n",
    "print(\"% Accuracy linear\", 100*clf.score(Xtransf_test, ytransf_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td><b>[0.25 points]</b> As you can see, both SVMs (nonlinear and linear) obtain the same result. Why?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td> For simplicity, we have compute de KDA projection on the whole circles dataset; however, in practice and real-world secarios KDA directions must be computed <b> only using the training data</b>.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> What is the dimension of the transformed data in the kernel space? Why? Plot the projected test data using matplotlib and differentiating the points in two classes with different colors. Do you think KDA get a good separation? HINT: It may be useful to see what we did to plot circles and moons datasets. You may also need to fill some dimensions with zeros. </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One step further on  Na√Øve Bayes\n",
    "\n",
    "It's time to move to Na√Øve Bayes. We'll use again the iris dataset, so we are going to generate the training and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Train a Na√Øve Bayes classifier over the iris data (training partition) using the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\">Scikit Learn implementation</a> with the default parameters. Print the class counts, class priors, and the means and variances for the gaussians associated to each variable.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td> Note that we are using the Gaussian Na√Øve Bayes implementation because all the attributes in the iris dataset are continuous.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.75 points]</b> Compute the class priors, and the means and variances for each gaussian using numpy. Print them and make sure they match with the results provided by the Sklearn implementation of GaussianNB.Some numpy functions that may be useful are <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html\">np.sum</a>, <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html\">np.mean</a> and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.var.html\">np.var</a>.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to make predictions. Instead of using the `predict` function in GaussianNB, we are going to use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.predict_log_proba\">predict_log_proba</a> function that returns log-probability estimates for the test vector X. It is better to avoid numerical issues when computing the posterior probabilities in Na√Øve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.25 points]</b> Compute the predicted log-probabilities over the test partition using the GaussianNB Sklearn classifier and the predict_log_proba function.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = gnb.predict_log_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td> <b>[0.5 points]</b> Let's try to reproduce the predictions made by Scikit Learn. Using the class priors and the gaussians means and variances you computed, estimate the log probabilities for the first sample in the test partition (X[0:1,:]), and make sure that these probabilites are the same as those obtained in the previous exercise. It may be useful to use the Python implementation for gaussians in <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\"> scipy.stats.norm</a>, and the function <b>logpdf</b> to estimate the probability of a datapoint. Recall that SCikit Learn normalizes the output probabilitiues by their maximm value.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>The same computations can be done with the other Na√Øve Bayes implementations in Scikit Learn: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">MultinomialNB</a>, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\"> BernoulliNB</a>, and <a href=\"https://scikit-learn.org/stable/modules/naive_bayes.html\">Categorical NB</a>. <b>Make sure you also understand the fundamentals behind these implementations!</b></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>In the previous exercise, we have not created training/validation/test partitions (we only used training and test partitions) since we did not adjust any hyperparameters in the classifiers. Please, remember that you need to generate these three datasets in case you need to perform some kind of hyperparameter optimization.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real-world example: default of credit card clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply what we've learnt over a real-world dataset. We will work with an adapted version of a dataset regarding credit card unpayment in Taiwan. This dataset is publicly available at the <a href=\"https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\">UCI Machine Learning Repository</a>, but the dataset that we are going to use is modified version of the original one, and it is stored in the file `default_credit.csv`.\n",
    "\n",
    "In the UCI website you can find detailed information about the dataset. In general, the idea is to predict credit card unpayment (variable `X24_default payment next month`) in the following month at which the data is referred to. Each row in the dataset (sample/pattern) represents a customer, and the following variables (predictors/attributes) are available for each customer:\n",
    "\n",
    "* **X1**: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.\n",
    "* **X2**: Gender (male;female).\n",
    "* **X3**: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "* **X4**: Marital status (married;single;others).\n",
    "* **X5**: Age (year).\n",
    "* **X6 - X11**: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n",
    "* **X12-X17**: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.\n",
    "* **X18-X23**: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. \n",
    "\n",
    "\n",
    "Features names are `X_i` followed by a descriptive text for the variable.\n",
    "\n",
    "The following code loads the dataset as a Pandas dataframe, and obtains the following two dataframes:\n",
    "* **df_y**: dataframe with a single column with the binary target variable `X24_default payment next month`(0: paid / 1:unpaid).\n",
    "* **df_X**: dataframe with columns corresponding to the predictors.\n",
    "\n",
    "The last line of code shows the names of the independent variables (predictors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>The following code also removes categorical features in order to make the application of LDA, QDA and Na√Øve Bayes straightforward.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"default_credit.csv\",sep=\";\")\n",
    "\n",
    "\n",
    "# Remove categorical features\n",
    "data.drop(columns=['X2_GENDER', 'X3_EDUCATION', 'X4_MARITAL_STATUS','X6_PAY_0', 'X7_PAY_2', 'X8_PAY_3', 'X9_PAY_4', 'X10_PAY_5',\n",
    "       'X11_PAY_6'], inplace=True)\n",
    "\n",
    "# Dataframe with the target variable\n",
    "df_y = data['X24_default payment next month']\n",
    "\n",
    "\n",
    "# Dataframe with the predictors (all features except for the target variable)\n",
    "df_X = data.drop('X24_default payment next month',axis=1)\n",
    "\n",
    "print(df_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "<tr><td width=\"80\"><img src=\"img/exclamation.png\" style=\"width:auto;height:auto\"></td><td>The following code uses the package Pandas Profiling, which is very useful for exploring our data.</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(data, title=\"Pandas Profiling Report\")\n",
    "\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "     <b>[0.25 points]</b> Compute what is the prior probability of the positive class. Taking into account the obtained result, what metrics would you use to measure the performance of your classification model? Why?\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we will use the **Area Under the Curve (AUC)** to measure the model performance. This metric is implemented by the function <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\">roc_auc_score</a> in Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "     <b>[0.25 points]</b> Make a random training/test partition of the dataset using 80\\% of data for traning, and 20\\% of data for test. Remember to set a random seed to some number in order to have reproducible results across different executions.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "     <b>[0.5 point]</b> Train LDA, QDA, and GaussianNB classifiers with the default parameters over the train set, and evaluate the performance of the classifiers over the test partition using the <b>area under the curve</b> as metric. Which models outperform a random classifier? Explain your answers.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "     <b>[0.5 points]</b> Now, evaluate the performance of the LDA, QDA, and GaussianNB classifiers over the training partitions using the <b>area under the curve</b> as metric. Do you detect any case of underfitting/overfitting? Explain your answers.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    " <tr><td width=\"80\"><img src=\"img/question.png\" style=\"width:auto;height:auto\"></td><td>\n",
    "     <b>[1 point]</b> In the theoretical part of the course, we saw how the different methods (LDA, QDA, Na√Øve Bayes) make different assumptions on the data. Determine whether these assumptions are satisfied for the real-world dataset. According to your results, would you expect the obtained performance? Why? HINT: The function <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html\">numpy.corrcoef</a> as well as the following code to get the density function of a variable for each class may be useful.\n",
    " </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that shows the density function of feature 'X1_LIMIT_BAL' with respect to the target variable 'X24_default payment next month'\n",
    "plt.figure(figsize=(10,5))\n",
    "df.groupby('X24_default payment next month', as_index=False)['X1_LIMIT_BAL'].plot(kind='kde',title='X1_LIMIT_BAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your answers to questions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "                          THIS IS THE END OF THE NOTEBOOK<br>\n",
    "~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.<br>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
